\documentclass{article}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{color}
\usepackage{listings}
\usepackage{multicol}


\topmargin0.0cm
\headheight0.0cm
\headsep0.0cm
\oddsidemargin0.0cm
\textheight23.0cm
\textwidth16.5cm
\footskip1.0cm

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\makeatletter
\def\and{%
  \end{tabular}%
  \hskip 0.0001em \@plus.17fil\relax
  \begin{tabular}[t]{c}}

\setlength\parindent{0pt} % Removes all indentation from paragraphs

\begin{document}

\title{\textbf{WACC 20: Reflection and Extension} \\ Imperial College London \\ Autumn}
\author{
  Paul Li\'etar\\
  \texttt{pl2113}
  \and
  Leanne Lyons\\
  \texttt{ll5914}
  \and
  Jaime Rodr\'iguez\\
  \texttt{jr1314}
  \and
  Ignacio Navarro\\
  \texttt{in714}
}
\date{}

\maketitle

\section{Product}
Analysis and critical evaluation of the quality of WACC.
Our WACC compiler has been written in Haskell. This has allowed us to use powerful pattern matching and monads to write our compiler. Being extremely critical, we think we have built a fantastic compiler; it passes all the test cases, it has some great extensions and most importantly it does it fundamental job of preserving the meaning of the program being compiled whilst converting it into WACC.

The layout of our WACC compiler is very clear and concise, within our source folder you can easily see what is required for the Frontend, the Backend and the extension. Each file has one meaningful purpose e.g. our Parser.hs parses input, our SemCheck.hs carries out semantic checking. 

The fact that our compiler is well formatted means that it is quite easily extendible if you wanted to add to the WACC language. Adding additional statements would not cause any conflicts with the existing program; all you need to do is add it to the parser, add the correct semantic check and then add how you want the code to be generated using the IR (Intermediate Representation) we have already defined. 

We changed the register allocation in our extension to produce better code. This, however, means that our final register allocation is a bit slower than what we initially committed for the backend. This slowdown actually causes LabTS to timeout on one of our test cases so this is definitely something we would need to address if rebuilding the compiler.  

\section{Project Management}

\subsection{Organisation}
The way our group kept organised throughout this project was through Git and WhatsApp. 

The organisation of our group got better and smoother as the project advanced. At the beginning the division of the Frontend was between the three parts: lexer, 
parser, and semantic checker. However, we weren't fully aware that each were not equivalent in terms of work, so for example the team members that worked on the parser worked harder than the ones that worked on the lexer. However we changed that on the Backend. Although inevitably we couldn't all contribute to the Backend in the same degree (as the expertise of some team members were far beyond others), we did know how to push each of the team members to work their hardest on each assignment. By the start of the extension, we were all on the same page and very organised. We decided to part ways on the extension and assigned a particular extension to each team member according to our level. This made the extension part of the project very pleasant to work on, as each team member had 
free room to play around but forced to learn more from the code written by others, since most extensions need the team member to modify many files. Hence Paul worked on concurrency and register spilling, Jaime worked on garbage collection, Ignacio worked on extending pairs to tuples, and Leanne improved the if statements, adding a switch statement. 

\subsection{Use of Git}
Our use of git definitely improved as the project advanced. We tried our best to always have meaningful commit messages and we would also include the initials of anyone that helped with that commit. For each milestone of the compiler we had several different branches. For example, our Frontend had separate branches for the parser, lexer and semantic checker, our backend had separate branches for code generation and ARM generation and for our extension we had separate branches for each individual extension. This made it very easy to keep track of what everyone had done and it also meant that our master branch would always successfully build since we did not push to master until our respective branches behaved correctly. Paul's invaluable knowledge and experience with git helped the other team members become very much more fluent in it and we have all undoubtedly become much more acquainted with Git than we did with any other small lab project.

\subsection{Communication}
The communication in our group was excellent from the beginning to the end. With the aid of WhatsApp we were able to organise our meetings and at each meeting we would generally all work beside each other but on different parts. This was useful in that if anyone had any issues they could easily seek help from another teammate. 

\subsection{Group Structure}
Our group lacked a rigid structure in that someone would just decide to do something and work on it by themselves rather than coordinating the task between a couple group members. However, throughout the project, the distribution of work did improve so that one or two people were not doing the majority of the work.

\subsection{What we would do differently}
If we were to do this lab again we would try and decide earlier on a more even distribution of work taking in to account each person's different programming ability. We also think it would be important to have a distinct team leader to delegate tasks.

\section{Design Choices}
Design patterns used and design choices made throughout the project.

\section{Extensions}
\subsection{Register Spilling}
\subsection{Concurrency}
\subsection{Garbage Collection}
For our WACC compiler, we decided it would be very interesting to manage our own heap memory instead of needing explicit allocation and freeing commands. Thus, we researched the different types of Garbage Collectors that
are commonly implemented. We discarded Reference Counting garbage collectors since they cannot free cyclic objects. Then we looked at different implementations of Mark-Sweep and Copying Collectors and we chose to design a Copying Collector that required less frequent calls to copy. This is how the Mostly-Copying Collector came into play; a more efficient version of the Copying Collector that minimises the number of copy calls. Our garbage collector is not Generational and does not Compact elements in the heap. Also instead of an Incremental collector, ours requires a Stop The World (STW) pause. However, the biggest defficiency that our WACC Collector has is the fact that it cannot interact with resumable functions properly. The collector is fully precise on the heap, where as resumable functions allocate space on the heap without type information, which means garbage collection is not correct when both of them interact. This is a result of the small time frame within which we have worked. Thus, Gargage Collection and resumable functions need to work independently. 



We have based our design on a paper by Antony L Hosking ``Portable, Mostly-Concurrent, Mostly-Copying Garbage Collection for Multi-Processors''. A code fragment from that paper follows. 

\noindent\begin{minipage}{.46\textwidth}
\begin{lstlisting}[caption=code 1,frame=tlrb, mathescape]{Name}
proc promote(p,color) ≡ 
  white := white \{p}; 
  color := color $\cup$ {p}.

proc move(l) ≡ 
  r := l$\uparrow$; // r is the object pointed to by l
  if page (r) $\in$ white then 
    if r' = nil then
      r' := copy(r);
      grey := grey $ \cup $ {page(r′)}
    end;
  l$\uparrow$ := r′
  end.


\end{lstlisting}
\end{minipage}\hfill
\begin{minipage}{.45\textwidth}
\begin{lstlisting}[caption=code 2,frame=tlrb, mathescape]{Name}
proc trace() ≡ 
  foreach p $\in$ grey do
    grey := grey\{p};
    foreach l $\in$ pointer locations(p) do
      move (l)
    end;
    black := black $ \cup $ {p}
  end.

proc gc() ≡
  white := black; grey := {}; black := {}; 
  foreach r $\in$ AR where page(r) $\in$ white do
    promote(page(r), grey);
  end;
  trace();
  foreach p $\in$ white do free(p) end.
\end{lstlisting}
\end{minipage}

The heap is a blocked of aligned memory requested to the operating system. We divide the heap into Pages of a fixed
size. We chose the alignment boundary for the allocated memory to be the same as the page size, which allows us
to easily determine the page in which an object is allocated, given the object pointer (simply by rounding the pointer
down to the boundary alignment). In each page, we allocate objects until the page is full. In addition to the space 
to allocate the object, each object requires a header, where we keep vital information about it. The header includes the size of the object, information about its type and a forward reference for when the object is copied. This of course produces an overhead in the memory usage that is usually unnecessary in Non Garbage-Collected languages. 


The aforementioned paper and algorithm build up on Joel Bartlett's Mostly-Copying Garbage Collector. Although 
Bartlett's original implementation is a Generational collector we used a different 
algorithm based upon the previous pseudo-code implementation. This implementation uses Dijkstra's tricolouring 
abstraction to explain the classification of allocated pages in the heap. During normal allocation, only Black pages 
exist. All objects are thus allocated into Black pages. During Collection, all black pages are converted into White 
pages, which can be considered the ``condemned pages'', since all the remaining White pages at the end of the collection 
process will be collected and freed. Collection starts by scanning the stack and registers and promoting all the pages 
that are ambiguously referenced from them into the Grey set. After this is done, Grey pages are scanned for pointer references to other heap objects. When one such pointer is found, if it references a White page, then the object is copied into a grey page to avoid it getting destroyed, otherwise the reference is already in a Grey page and nothing needs to be done. After a full page is scanned, it is promoted to a Black (stable) page, representing that it contains live objects with no pointers to a white page. When the Grey Set is empty, all objects that are still ``alive'' are in Black pages. We thus free all remaining white pages and proceed with normal allocation.



\subsection{Improving If Statements}
We decided to extend the WACC language with additional conditional branching statements; `if' statements without an `else' branch and `switch' statements. Initially to implement the `if' statements without an `else' branch we defined a new data type called `StmtIfNoElse' which had only the condition and the `then' branch associated with it but this added a lot of repetitive code as it was extremely similar to our `StmtIf'. To overcome this problem, we made use of Haskell's Maybe monad to  define `if' statements to have a Maybe block. We then had to change Parser.hs to allow this new syntax and then we had to add the new semantics to SemCheck.hs. The difference in semantic checking between an `if' statement with an `else' branch and one without was that in the case where the `else' block was empty we had to show that the `if' statement never necessarily returns a value.  Finally we had to ensure the correct assembly code was generated which meant changing CodeGen.hs to have a case expression to deal with if the block contained a value (Just b) or was empty (Nothing).
For `switch' statements we had to define new keywords in lexer.x for `switch' and `case'. Then, similarly we had to add switch statements to our semantic checker and parser and edit how the assembly code is generated.
\subsection{Extending Pairs to Tuples}
One limited feature of WACC was the ability to deal with pairs instead of arbitrarily sized tuples. So a nice extension was to expand WACC to support tuples. The first step in this extension was to control the would-be redundancy if we kept all the code handling pairs and creating new code for tuples. So what was done was to naturally consider pairs as a subset of tuples. The only difference in code then would be on the lexer and the parser, but from the parser on we considered pairs as tuples and we only wrote code regarding tuples. The only problem then was accessing a pair with \texttt{fst} and \texttt{snd}. We made the decision that tuples were to be accesed in the same manner as arrays (i.e. \texttt{a[1]}), so then it  was an easy fix as we just considered \texttt{fst p = p[0]} and \texttt{snd p = p[1]}.

\begin{lstlisting}
begin
  int[] arr = [1,2,4,5];
  tuple(int, bool, char, int[]) tup = newtuple(1, false, 'c', arr);
  println tup[0]; # Prints 1.
  println tup[3][2] # Prints 4.
end
\end{lstlisting}

The second problem we encountered was that now arrays and tuples are accessed in the same manner. However, tuples can only be accesed with literal integers (so
then \texttt{int b = 2; println tup[b]} is ilegal but \texttt{int b = 2; println tup[2]} is legal in our extension) while arrays can be indexed with expressions. So we changed our design of arrays and merged the two into an IndexingElem instead of having TupleElem and ArrayElem. This helped solved the problem because now it was a matter of just pattern matching in the appropiate moment as we always carry around the type of the IndexingElem; either a tuple or an array.

\subsection{Future extensions}

\end{document}
